18:05:12 I don't even know if it's there for the Okay, I guess hello, everyone
18:05:52 What,
18:06:05 You might need to do it. let me see
18:11:52 Okay, let's just make sure he's recording this
18:12:03 Okay, hello, everyone. is that
18:12:13 Is it recording? Do you know does it say for you?
18:12:20 It says it says, recording, Yeah.
18:12:35 Do we have like a team name or now. no Okay, okay, Alright, let's just run it.
18:12:47 I will figure it out. We'll just trim it and shit
18:12:49 Hello, everyone! Hello! Class for our project. we did predicting Crab Age with machine learning.
18:12:57 And this is for the machine learning class of course, and in our group.
18:13:01 It's just me Brian l sarro and on my teammate, Justin hank on the next slide.
18:13:09 You'll see gonna start off with the abstract which is what we wrote in our paper.
18:13:14 It's a really simple, really straightforward machine learning can be used to protect the age of craps, and it can be more accurate than simply weighing a crap to estimate its age.
18:13:26 There's several different models that you can use through support back to regression.
18:13:33 That was actually the best model that we found to be most accurate in the experiment.
18:13:38 So more in depth on like the problem and Why, it's important. And the strategy that we came up with to like actually make a solution to to determining the age. first let's start with the problem so
18:13:54 It's quite difficult to actually determine the crabs age just because course there's smaller crabs, and there's bigger crabs.
18:14:04 You can kind of tell age that way, but when you when you get down to like the crabs actual molting cycle. one of the like features of a crab is it has a shell? right?
18:14:16 And it's hard or it's soft when it molds it's soft.
18:14:19 So, even if it's an adult it can be thought of as really young.
18:14:24 Because it's soft and it just molted So then like it's actually quite hard to determine the age.
18:14:31 So the reason why this is an issue, though, is because the failure to harvest the use crabs at like the like optimal time, causes a waste in business essentially and like higher costs.
18:14:45 And you're. just not gonna get like the the business like like, I guess efficiency that you would normally get, or that you could get.
18:14:54 So that's why, it's important even though there's.
18:14:59 Like beyond a certain age, there's negligible growth in the crowds physical characteristics and us like it's important to time.
18:15:05 The harvesting to reduce that cost and increase profit.
18:15:10 So our solution is prepare crab data from a data set that we found cycle and use it to train several different machine learning models.
18:15:24 So we were, you might be basically we take the crowd data like given physical characteristics of it.
18:15:29 And we're going to use those models to determine the age and crats. That's the next slide.
18:15:39 That one part the event shorter so i'll give a little bit more backgrounds on that.
18:15:45 So we're actually gonna assume in this prediction system that a crab is been sure and ready to harvest after 12 months.
18:15:55 They grow pretty fast, and they're pretty much fully grown ready to go brave harvest at 12 months.
18:16:02 So we're gonna actually ignore other features that affect the crabs harvest ability like such as eglene correct egg laying crabs.
18:16:10 We're also gonna predict age rather than weight since machine learning is more applicable. to actually predict it.
18:16:19 It's kind of like points to be just wait because I mean all the models that we would choose so like that we would implement like It's can you quite like accurate to protect the weight but age is what's
18:16:32 like more important in this problem at least, so as far as like going back to the data set that I mentioned that we got from cattle.
18:16:42 There's over a 1,000 samples it's closer to like about 3, almost 4,000 individual records.
18:16:52 So we have a different features for each crap There is sex, length, diameter, height, weight, shocked, weight, scare, weight, shell, weight, and age, of course, which is our classifier. and originally, like as you can see
18:17:07 it's a it all all like we have discrete and categorical, or or like we were thinking about like whether we should do a categorical classifier, or like a discrete classifier
18:17:23 or like a something that's like maybe even continuous to
18:17:28 But eventually, like, we decided that as far as like feature selection goes data pro pre processing everything like that which we'll see in these later slides, which feature like selection, we're gonna actually go
18:17:43 with So next slide. So yeah, for the data pre-processing, we actually converted sex to numerical values.
18:17:51 So that's to like make the data set actually have all the same data type.
18:17:57 And we also did a train to our test train split so used to seeing train tests.
18:18:05 But for that train test blow. We did test size 30%.
18:18:08 And train size 70%, of course. moving on to the rest of the.
18:18:16 So this leads to like feature selection what we did is we use the piercing correlation coefficients.
18:18:24 So we're gonna ignore gender altogether or ignore sex and keep all the other features so greater than 0 Point 5 is pretty high correlation.
18:18:34 So we we use this to actually, determine the features as far as like the methodology goes like if you want to get like a general kind of sense of like, the projection system like workflow you, start with like the problem
18:18:52 domain which I mentioned in the very beginning, which is the age prediction of crabs.
18:18:57 We're gonna like filter it out and like we're gonna like slowly branch out to like determine like the various like, I would say like introductory features like base features or base like process steps which
18:19:13 is data representation. So we get the data set. we did some pre-processing to it.
18:19:20 We get a function or like we determine like the training and test set.
18:19:28 And then, basically, what we're, gonna do is we're gonna use a metric to actually determine like what like what kind of like function it is or like.
18:19:39 We evaluate it essentially and we use average error rate.
18:19:41 So that's on both training and test, data. So then eventually we came we decided on 3 algorithms K Nn: which is K nearest neighbors Mlr.
18:19:54 Which is multiple linear Russian and Svm support vector machine and
18:20:00 So you get 3 different predictive models, and this combines makes the protection system.
18:20:08 The aggregate model results. and finally, you get either useful predictions, which is like what you wanna see like a good prediction system, or instead, you also get like domain insights as well. so maybe like other interests
18:20:24 and like how you can treat this system to I guess to get different insights.
18:20:30 Get different. inferences. Why not moving on to the methodologies of like the main 3 models that we have?
18:20:41 So we have a 3 machine learning models and one baseline model.
18:20:44 So the one baseline model just being the linear aggression with weight.
18:20:49 So it's just simple linear regression just line and it's weight versus H.
18:20:58 So I guess that's kind of like the introductory like almost test model.
18:21:03 It's very, very simple that we move on to kanarest neighbors multiple new regressions board vector regression just like I mentioned earlier.
18:21:12 And we use the psychiatr. learn python libraries to implement those moving on to Excellent.
18:21:19 So the first one we have multiple linear regression. So for this the methodology that we went with, or that, I decided on is just straightforward, simple vanilla feature selection, which is we remove the gender.
18:21:34 So we took the data set did some feature selection.
18:21:36 We did the Pearson coefficient correlation test, basically.
18:21:43 And then do the train test split train The model evaluate the results, and we got the measured.
18:21:50 The average error moving on i'll hand it over to my my teammate Justin, and he's gonna go over the methodology and the rest of the presentation.
18:22:02 Alright, Thank you, Brian. So for the metadata for the Knn.
18:22:06 We need to find what value to use for K. And in order to do that we had to plot the average error from 0 to 34 K.
18:22:15 And basically figure out which value was the best in that one.
18:22:19 So you can see on the graph on the right. That case seems to plateau around 1510, or 15, and around like 20 is where it starts to get kind of even.
18:22:33 So we decided on K. equals 20 for k value alright, and moving on to the results.
18:22:43 You can see this is kind of a messy graph of every single model laid on top of each other.
18:22:48 We'll go over each model in detail and moving on to our baseline.
18:22:58 So again, this was linear linear regression, where we only use weight to determine the age.
18:23:07 And the average error was about basically 2 months you can see on the graph on the right that a lot of the times the peaks where the problem areas and it wasn't able to correctly predict those just overall the model then it
18:23:24 predicted. Very well,
18:23:30 And then moving on to our K. and N. model. Already you can see it outperformed.
18:23:35 The baseline, and it was only it was actually slightly in the worst.
18:23:40 Out of 3 machine learning models with an average error of 1 point.
18:23:46 6 months. It was not very accurate at predicting the ages that were under 12 months.
18:23:53 But it was okay at predicting, like some of the peaks, you can see the data set around like 33.
18:24:00 It was able to correctly predict that one overall.
18:24:05 It was a it was a decent model
18:24:09 So for multiple linear regression. this one also outperformed the baseline it's very similar to linear regression where it uses the features, and it tries to fit a line through it.
18:24:24 But of course, since this one has more features it Well, I shouldn't say always, but it usually does better.
18:24:31 So, and it was also better at predicting mature ages above 12 months overall.
18:24:38 A good model, and its average error was about one and a half months again.
18:24:44 You can see the graph on the right
18:24:52 So for the results for the support vector regression. This one also outperformed the baseline, and it was pretty accurate at predicting just under 12 months compared to the last one which is better above 12 months.
18:25:05 This one actually had the least amount of error where the average error was 1 point.
18:25:11 4 months again. you can see it on the right for most of the the the troughs.
18:25:17 It actually fit those very well to no model. It had trouble predicting some of the peaks overall.
18:25:25 All of the models had trouble predicting the peaks.
18:25:31 So yeah, those are our models. And this is just a table summarizing the results. A simple linear regression which was the baseline, had an error of 1.9.
18:25:46 So overall our models. beat that by about 0 point.
18:25:48 3 months errors and the support vector regressing the best at 1.4.
18:25:55 But it was very close to the other 2
18:26:03 So in conclusion, we learned that machine learning did outperform the simple linear regression, and on average, the models had an error of about 1 point. 5 months compared to the baseline which had 2.0 months supportive
18:26:19 regression did have the slightly but multiple linear regression and K.
18:26:25 Nearest neighbor. We're also good predictors so overall. You could say that any of the models could be used to predict grab age pretty well the predictions were good up until 12 months and we think that the reason this
18:26:40 is because after 12 months, when the crabs reach maturity, the features no longer, for example, the weight the height they no longer increase.
18:26:49 So that's probably Why, our models had trouble predicting Crabby just past 12 months.
18:26:55 So in the future we want to improve our model anytime.
18:27:01 The features were above 12 months, we would prize as hard, set them to 12 months, and it would probably have less error overall
